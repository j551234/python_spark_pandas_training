{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb82576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aafd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_file_name(folder_path):\n",
    "    category_file_list = []\n",
    "    folder_content = os.listdir(folder_path)\n",
    "    for item in folder_content:\n",
    "        if (item.endswith(\"_category_id.json\")):\n",
    "            category_file_list.append(item)\n",
    "    return category_file_list\n",
    "\n",
    "\n",
    "def get_video_file_list(folder_path):\n",
    "    video_file_list = []\n",
    "    folder_content = os.listdir(folder_path)\n",
    "    for item in folder_content:\n",
    "        if (item.endswith(\"videos.csv\")):\n",
    "            video_file_list.append(item)\n",
    "    return video_file_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a6ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"/opt/archive\"\n",
    "\n",
    "category_list = get_category_file_name(test_path)\n",
    "video_list = get_video_file_list(test_path)\n",
    "\n",
    "# skip=set((\"JP\",\"KR\",\"MX\",\"RU\"))    \n",
    "\n",
    "# for skip_item in skip:\n",
    "#     for  category_item in category_list :\n",
    "#         if skip_item in category_item:\n",
    "#             category_list.remove(category_item)\n",
    "#             continue\n",
    "\n",
    "# for skip_item in skip:\n",
    "#     for  video_item in video_list :\n",
    "#         if skip_item in video_item:\n",
    "#             video_list.remove(video_item)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9386f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 11:54:27,245 WARN util.Utils: Your hostname, james-MS-7A69 resolves to a loopback address: 127.0.1.1; using 10.100.0.179 instead (on interface enp0s31f6)\n",
      "2021-10-29 11:54:27,245 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/james/anaconda3/envs/py_spark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2021-10-29 11:54:27,566 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-10-29 11:54:29,220 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/opt/hadoop/etc/hadoop\"\n",
    "\n",
    "\n",
    "# 本機\n",
    "# spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "#                     .appName('spark-yarn') \\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "\n",
    "# spark 叢集\n",
    "# spark = SparkSession.builder.master(\"spark://master:7077\") \\\n",
    "#                     .appName('py_spark') \\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "# yarn\n",
    "spark = SparkSession.builder.master(\"yarn\") \\\n",
    "                    .appName('spark-yarn') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc96b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_df(file_name):\n",
    "    df = spark.read.option(\"multiline\",\"true\").json(file_name)\n",
    "    items_df =df.rdd.flatMap(lambda x:x[\"items\"]).toDF()\n",
    "    title_df = items_df.select('id',items_df.snippet.title.alias(\"category_name\"))\n",
    "    return title_df\n",
    "\n",
    "def get_video_df(file_name):\n",
    "    data_df =spark.read.option(\"header\",True).csv(file_name)\n",
    "    area = file_name[file_name.index(\"archive/\") + 8 :file_name.index(\"videos.csv\")]\n",
    "    data_df =data_df.withColumn(\"area\",  lit(area))\n",
    "    return data_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 抓出所有影片和分類\n",
    "test_path = \"/opt/archive/\"\n",
    "\n",
    "        \n",
    "csvDF=get_video_df(test_path + video_list[0])\n",
    "\n",
    "for video_item in video_list[1:] :\n",
    "    csvDF=csvDF.union(get_video_df(test_path + video_item))\n",
    "    \n",
    "    \n",
    "jsonDF=get_category_df(test_path + category_list[0])\n",
    "for category_item in category_list[1:] :\n",
    "    jsonDF=jsonDF.union(get_category_df(test_path + category_item))\n",
    "\n",
    "\n",
    "jsonDF=jsonDF.dropDuplicates()\n",
    "csvDF =csvDF.withColumn(\"category_id\",  col('category_id').cast(IntegerType()))\n",
    "\n",
    "jsonDF =jsonDF.withColumn(\"id\",  col('id').cast(IntegerType()))\n",
    "joinDF=csvDF.join(jsonDF, csvDF['category_id'] == jsonDF['id'] )   \n",
    "\n",
    "joinDF.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51624d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joinDF.write.format(\"hive\").mode(\"overwrite\").saveAsTable(\"test_tableA\")\n",
    "# spark.sql(\"select * from test_tableA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41053c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 列出 所有 rap 影片 的dislike 數目\n",
    "rap_df=joinDF.filter(joinDF.tags.like(\"%rap%\"))\n",
    "rap_df[['title','dislikes']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 找出發布時間與地區 和 喜歡人數的線性關係\n",
    "relation_df =joinDF\n",
    "relation_df =relation_df.withColumn(\"likes\",  col('likes').cast(IntegerType()))\n",
    "relation_df =relation_df.withColumn(\"publish_time\", hour(relation_df[\"publish_time\"]))\n",
    "relation_df =relation_df.groupBy(\"area\",\"publish_time\").sum(\"likes\")\n",
    "relation_df =relation_df.orderBy(col(\"area\"),col(\"publish_time\"))\n",
    "\n",
    "\n",
    "pd_df=relation_df.toPandas()\n",
    "\n",
    "area_array =pd_df['area'].unique()\n",
    "\n",
    "display(area_array)\n",
    "\n",
    "for i in area_array :\n",
    "    pd_each = pd_df[pd_df['area'] ==i]\n",
    "    area = i\n",
    "    pd_each.plot(x='publish_time', y ='sum(likes)',label =area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c09930",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 排序出最多人不喜歡的前十名種類影片\n",
    "dislike_df =joinDF\n",
    "dislike_df =dislike_df.groupBy('title').agg(f.sum('dislikes').alias('dislikes')).sort(f.col('dislikes').desc()).head(10)\n",
    "spark.createDataFrame(dislike_df).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aafd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 排序出前十名的類別 喜歡 和 不喜歡的比例 擁有最大的變異數\n",
    "var_df =joinDF\n",
    "var_df=var_df.withColumn(\"ratio\",f.col(\"likes\")/(f.col(\"likes\")+f.col(\"dislikes\")))\n",
    "\n",
    "var_df=var_df.groupby('category_name')\\\n",
    "    .agg(f.variance(\"ratio\").alias(\"ratio\"))\\\n",
    "    .sort(f.col('ratio').desc())\n",
    "\n",
    "display(var_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebbe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84146ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
